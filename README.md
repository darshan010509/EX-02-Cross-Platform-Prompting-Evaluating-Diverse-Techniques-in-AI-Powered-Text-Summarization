AIM

To evaluate and compare the effectiveness of prompting techniques (zero-shot, few-shot, chain-of-thought, and role-based) across major AI platforms (ChatGPT, Gemini, Claude, Copilot) in performing text summarization of a 500-word technical article titled “The Basics of Blockchain Technology”.
The evaluation focuses on accuracy, coherence, simplicity, speed, and user experience to identify the best platform + prompt strategy combination for educational content summarization.

Algorithm
Step 1: Define Scope and Objective

1.1 Clearly state the research goal:

Compare prompting strategies across multiple AI platforms for a summarization task.

Determine which combination performs best for undergraduate-level content.

1.2 Define evaluation criteria:

Accuracy: How factually correct and complete the summary is.

Coherence: Logical flow and readability.

Simplicity: Understandability for undergraduate students.

Speed: Response time for generation.

User Experience: Ease of use, interface clarity, and stability.

1.3 Fix the content source:

500-word article “The Basics of Blockchain Technology”.

Step 2: Prepare the Prompting Techniques

2.1 Zero-shot prompt

“Summarize the following 500-word article on Blockchain Technology for undergraduate students.”

2.2 Few-shot prompt

Add 1–2 examples of summaries of similar articles, then request a summary of the new article.

2.3 Chain-of-Thought (CoT)

“Think step-by-step. First list key concepts from the article, then derive a structured summary.”

2.4 Role-based prompt

“You are an academic content curator. Summarize the article in simple terms for undergraduate students…”

Step 3: Execute Summaries on Each AI Platform

3.1 Select platforms:

ChatGPT, Gemini, Claude, Copilot.

3.2 For each platform, run the same 4 prompt types:

Platform	Zero-shot	Few-shot	CoT	Role-based

3.3 Record:

Generated summary

Time taken

Token usage (if visible)

Step 4: Evaluate the Outputs

4.1 Create a scoring rubric (scale 1–5):

Accuracy

Coherence

Simplicity

Speed

User Experience

4.2 Analyze summaries:

Match summary content with the original article.

Check flow, readability, difficulty level.

Note any hallucinations or missing information.

4.3 Populate evaluation table.

Step 5: Compare & Conclude

5.1 Compute average scores for each:

Platform

Prompt type

Combination (e.g., ChatGPT + CoT)

5.2 Identify best-performing combination based on highest total score.

5.3 Explain why it performed best.

5.4 Provide final recommendation for the educational content team.

Step 6: Report the Findings

6.1 Format the evaluation into a structured document/report.
6.2 Include:

Tables

Charts (optional)

Example outputs

Final ranking
6.3 Export as PDF for team circulation.

Result (Example Output Summary)

Below is an example output based on a hypothetical evaluation:

Summary of Findings (Example)

After running all four prompting strategies across the four AI platforms, the results indicated the following:

Best Overall Combination:

⭐ ChatGPT + Chain-of-Thought Prompting

Reasoning:

Highest Accuracy (5/5): Included all relevant components of blockchain—blocks, decentralization, hashing, consensus, immutability.

Excellent Coherence (5/5): Produced a structured and logically connected summary.

High Simplicity (4.5/5): Concepts explained in student-friendly language.

Fast Response (4/5): Summary returned within 3 seconds.

Great UX (5/5): Simple interface, stable performance.

Platform-wise Performance (Example Scores)

(Scale 1–5)

Platform	Zero-shot	Few-shot	CoT	Role-based
ChatGPT	4.2	4.6	4.9	4.7
Gemini	3.8	4.1	4.5	4.3
Claude	4.0	4.4	4.7	4.5
Copilot	3.5	3.9	4.2	4.0


